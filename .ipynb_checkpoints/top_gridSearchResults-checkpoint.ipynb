{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Librería Genética\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "# Subfunciones de estimadores\n",
    "from sklearn.base import clone\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py][30]\n",
    "from sklearn.base import is_classifier\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py][535]\n",
    "from sklearn.model_selection._validation import _fit_and_score\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_validation.py][346]\n",
    "from sklearn.model_selection._search import BaseSearchCV\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py][386]\n",
    "from sklearn.model_selection._search import check_cv\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_split.py][1866]\n",
    "from sklearn.model_selection._search import _check_param_grid\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py][343]\n",
    "from sklearn.metrics.scorer import check_scoring\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/scorer.py][250]\n",
    "from sklearn.utils.validation import _num_samples\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py][105]\n",
    "from sklearn.utils.validation import indexable\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py][208]\n",
    "from multiprocessing import Pool, Manager, cpu_count\n",
    "\n",
    "# Selección para estimadores\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Metricas para estimadores\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Estimadores\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Ensembles algorithms\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_config():\n",
    "    top_conf = {}\n",
    "    top_conf['LogisticRegression']\t=\tnp.array([4, 1, 2, 3, 1])-1    \n",
    "    top_conf['LinearDiscriminantAnalysis']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    top_conf['GaussianNB']\t=\tnp.array([4, 1, 2, 6, 1])-1\n",
    "    top_conf['MLPClassifier']\t=\tnp.array([4, 1, 4, 6, 1])-1\n",
    "    top_conf['SVC']\t=\tnp.array([6, 1, 6, 6, 1])-1#np.array([6, 6, 3, 3, 5])-1 #3\n",
    "    top_conf['DecisionTreeClassifier']\t=\tnp.array([6, 1, 3, 6, 5])-1#np.array([6, 1, 3, 3, 5])-1 #2\n",
    "    top_conf['KNeighborsClassifier']\t=\tnp.array([6, 1, 3, 6, 1])-1\n",
    "    top_conf['RandomForestClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    top_conf['ExtraTreesClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    top_conf['GradientBoostingClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    top_conf['AdaBoostClassifier']\t=\tnp.array([6, 1, 3, 6, 5])-1#np.array([6, 1, 3, 3, 1])-1#6\n",
    "    top_conf['VotingClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    return top_conf\n",
    "    \n",
    "def set_models():\n",
    "    rs = 1\n",
    "    models = []\n",
    "    models.append(('LogisticRegression', LogisticRegression(C=0.9, multi_class='multinomial', solver='newton-cg', warm_start=True)))\n",
    "    # LDA : Warning(Variables are collinear)\n",
    "    models.append(('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis(solver='eigen')))    \n",
    "    models.append(('GaussianNB', GaussianNB(priors=None)))\n",
    "    models.append(('MLPClassifier', MLPClassifier(hidden_layer_sizes=100, alpha=1e-05, activation='tanh', solver='adam', learning_rate='invscaling')))\n",
    "    models.append(('SVC', SVC(random_state=rs, C=10, kernel='poly', decision_function_shape='ovo')))\n",
    "    models.append(('DecisionTreeClassifier', DecisionTreeClassifier(random_state=rs, max_features=None, splitter='best', criterion='entropy', class_weight=None, max_depth=10)))\n",
    "    models.append(('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='ball_tree')))\n",
    "    \n",
    "    models.append(('RandomForestClassifier', RandomForestClassifier(random_state=rs, max_depth=20, class_weight=None, max_features='sqrt', criterion='gini', warm_start=True, n_estimators=30)))\n",
    "    # Bagging and Boosting\n",
    "    # models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=150)))\n",
    "    models.append(('ExtraTreesClassifier', ExtraTreesClassifier(random_state=rs, max_depth=5, class_weight='balanced_subsample', min_samples_leaf=1, max_features='sqrt', max_leaf_nodes=9, criterion='gini', n_estimators=15)))\n",
    "    \n",
    "    # models.append(('AdaBoostClassifier', AdaBoostClassifier(DecisionTreeClassifier())))\n",
    "    \n",
    "    models.append(('GradientBoostingClassifier',GradientBoostingClassifier(random_state=rs, max_features='sqrt', learning_rate=0.1, max_depth=3, n_estimators=100)))\n",
    "    models.append(('AdaBoostClassifier', AdaBoostClassifier(DecisionTreeClassifier(random_state=rs, max_features=None, splitter='best', criterion='entropy', class_weight=None, max_depth=10), \n",
    "                                                            n_estimators=500, algorithm='SAMME', learning_rate=2.0, random_state=rs)))\n",
    "    # models.append(('GradientBoostingClassifier', GradientBoostingClassifier()))\n",
    "    # Voting\n",
    "    estimators = []\n",
    "    estimators.append((\"Voting_GradientBoostingClassifier\",GradientBoostingClassifier(random_state=rs, max_features='sqrt', learning_rate=0.1, max_depth=3, n_estimators=100)))\n",
    "    estimators.append((\"Voting_ExtraTreesClassifier\", ExtraTreesClassifier(random_state=rs, max_depth=5, class_weight='balanced_subsample', min_samples_leaf=1, max_features='sqrt', max_leaf_nodes=9, criterion='gini', n_estimators=15))) \n",
    "    voting = VotingClassifier(estimators, voting='soft')\n",
    "    models.append(('VotingClassifier', voting))\n",
    "    return models\n",
    "\n",
    "test_size = 0.2\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "frecuencias = []\n",
    "names_ = ['Be01', 'Be02', 'Be03', 'Be04', 'Be05', 'Sector']\n",
    "\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x01'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x02'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x03'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x04'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x05'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x06'))#, names=names_))\n",
    "\"\"\"\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx1.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx2.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx3.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx4.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx5.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx6.csv', names=names_))\n",
    "\"\"\"\n",
    "num_jobs=cpu_count()\n",
    "#estimadores = set_models()\n",
    "#configuracion = set_models()\n",
    "#salida = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find distance error al 0.2%\n",
    "def distance_error(estimator, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 7)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    # coord pred\n",
    "    x1 = np.int32((y_pred + 2) % 3)\n",
    "    y1 = np.int32((y_pred - 1) / 3)\n",
    "    # coord real\n",
    "    x2 = np.int32((y_test + 2) % 3)\n",
    "    y2 = np.int32((y_test - 1) / 3)\n",
    "    # pasar variacion a distancias metros\n",
    "    vx = np.abs(x1 - x2)*1.5\n",
    "    vy = np.abs(y1 - y2)*1.5\n",
    "    #vx = vx*0.5 + (vx-1)*(vx>0)\n",
    "    #vy = vy*0.5 + (vy-1)*(vy>0)\n",
    "    # pitagoras\n",
    "    err_distance = np.sqrt(vx*vx + vy*vy)\n",
    "    return err_distance\n",
    "\n",
    "#def _createDataset(frecuencias, values, seed = 7):\n",
    "def _createDataset(frecuencias, values):\n",
    "    from sklearn.utils import shuffle as shuff\n",
    "    # crear dataset\n",
    "    names_ = frecuencias[0].columns.values\n",
    "    seed = 7\n",
    "    # reestructuracion\n",
    "    salida_final = pd.DataFrame(columns=names_)\n",
    "    for sec in range(1,16):\n",
    "        dataset = pd.DataFrame(columns=names_)\n",
    "        corte = min([frecuencias[i][frecuencias[i]['Sector']==sec].shape[0] for i in values])\n",
    "        #l = [frecuencias[i][frecuencias[i]['Sector']==sec].shape[0] for i in values]\n",
    "        #corte = max(l)\n",
    "        #tx=l.index(max(l))\n",
    "        tx = 0\n",
    "        dataset[names_[tx]] = dataset[names_[tx]].append(frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx]])\n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "        for tx in range(1,5):\n",
    "            dataset[names_[tx]] = frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx]].reset_index(drop=True)\n",
    "        dataset[names_[tx+1]] = frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx+1]].reset_index(drop=True)\n",
    "        # join parts\n",
    "        salida_final = salida_final.append(dataset)\n",
    "    # shuffle dataset\n",
    "    salida_final = shuff(salida_final, random_state=seed).reset_index(drop=True)\n",
    "    salida_final = salida_final.apply(pd.to_numeric)\n",
    "    # dataframe to X,y \n",
    "    X = salida_final[names_[:-1]]\n",
    "    y = salida_final[names_[-1]]\n",
    "    return X,y\n",
    "\n",
    "# The problem to optimize\n",
    "def getAccuracy( frecuencias, individual, estimator, score_cache, resultados ):\n",
    "\tX,y = _createDataset(frecuencias, individual)\n",
    "\tscore = 0\n",
    "\tscorer = \"accuracy\"\n",
    "\tname = str(estimator).split('(')[0]\n",
    "\tparamkey = name+str(np.int32(individual)+1)\n",
    "\tif paramkey in score_cache:\n",
    "\t\tscore = score_cache[paramkey]\n",
    "\telse:\n",
    "\t\tprint(\"Modeling ....\",name)\n",
    "\t\tkfold = KFold(n_splits=10, shuffle=False)\n",
    "\t\tcv_results = cross_val_score(estimator, X, y, cv=kfold, scoring=scorer)\n",
    "\t\t#print(name,\"  \",paramkey,\"   \")\n",
    "\t\t#print(len(X),\"  \",len(y),\"   \", kfold)\n",
    "\t\tscore = cv_results.mean()\n",
    "\t\tdesv = cv_results.std()\n",
    "\t\terror = distance_error(estimator, X, y)\n",
    "\t\tscore_cache[paramkey] = score\n",
    "\t\tdict_result = {'Modelo': name, 'Configuracion':np.int32(individual)+1, 'values': cv_results, 'Accuracy': score, 'stdAccuracy': desv, 'errorMetrico': np.mean(error), 'error': error }\n",
    "\t\tresultados.append(dict_result)\n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_order(dataframe_plot):\n",
    "    previos = ['LogisticRegression', 'LinearDiscriminantAnalysis', 'GaussianNB', 'MLPClassifier', \n",
    "               'SVC', 'DecisionTreeClassifier', 'KNeighborsClassifier', 'RandomForestClassifier', \n",
    "               'ExtraTreesClassifier', 'GradientBoostingClassifier', 'AdaBoostClassifier', 'VotingClassifier']\n",
    "    nuevos = ['LoR', 'LDA', 'GNB', 'MLP', 'SVC', 'DT', 'k-NN', 'RF', 'ET', 'GBM', 'AB', 'VC']\n",
    "    num_models = len(nuevos)\n",
    "    dataframe_plot = dataframe_plot[['Modelo', 'Configuracion', 'Accuracy', 'errorMetrico', 'values', 'error']]\n",
    "    for i in range(num_models):\n",
    "        dataframe_plot['Modelo'] = dataframe_plot['Modelo'].str.replace(previos[i], nuevos[i])\n",
    "        #df['Modelo'] = df['Modelo'].str.replace('LinearDiscriminantAnalysis','LDA')\n",
    "    sorterIndex = dict(zip(nuevos,range(num_models)))\n",
    "    #test\n",
    "    dataframe_plot['Model_Rank'] = dataframe_plot['Modelo'].map(sorterIndex)\n",
    "    dataframe_plot = dataframe_plot.sort_values(['Model_Rank'],ascending=True).reset_index(drop=True)[dataframe_plot.columns[:-1]]\n",
    "    return dataframe_plot\n",
    "\n",
    "#dataframe_plot = topDf\n",
    "def column_boxplot(dataframe_plot, column_plot, filename, box_bool=False):\n",
    "    %pylab inline\n",
    "    pylab.rcParams['figure.figsize'] = (14, 8)\n",
    "    dataframe_plot = rename_order(dataframe_plot)\n",
    "    nuevos = list(rename_order(topDf)['Modelo'])\n",
    "    num_models = len(dataframe_plot)\n",
    "    if column_plot == 'values':\n",
    "        y_label = 'Score'\n",
    "        x_label = 'Model'\n",
    "    else:\n",
    "        y_label = 'Error (m)'\n",
    "        x_label = 'Model Evaluated'\n",
    "    lista_plot = []\n",
    "    for i in range(num_models):\n",
    "        num_splits = len(list(dataframe_plot[column_plot])[i])\n",
    "        for j in range(num_splits):\n",
    "            d = {x_label:nuevos[i], y_label:dataframe_plot[column_plot][i][j]}\n",
    "            lista_plot.append(d)\n",
    "    #pd.DataFrame(lista_plot)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    if column_plot == 'values':\n",
    "        medians = np.round(list(dataframe_plot['Accuracy']),3)\n",
    "        ax_plot = sns.boxplot(data=pd.DataFrame(lista_plot), x=x_label, y=y_label, linewidth = 1.0)\n",
    "        tope = 1\n",
    "        var_tope = 0.02\n",
    "    else:\n",
    "        #ax_plot = sns.barplot(data=pd.DataFrame(lista_plot), x=x_label, y=y_label, linewidth = 1.0)\n",
    "        medians = np.round(list(dataframe_plot['errorMetrico']),3)\n",
    "        if box_bool==True:\n",
    "            ax_plot = sns.boxplot(data=pd.DataFrame(lista_plot), x=x_label, y=y_label, linewidth = 1.0)\n",
    "            tope = 7\n",
    "            var_tope = 0.2\n",
    "        else:\n",
    "            ax_plot = sns.barplot(data=pd.DataFrame(lista_plot), x=x_label, y=y_label, linewidth = 1.0)\n",
    "            tope = 2\n",
    "            var_tope = 0.1\n",
    "    plt.format='eps'\n",
    "    #\n",
    "    import matplotlib as mplt\n",
    "    ax_plot.set_xlabel(x_label,fontsize=18)\n",
    "    ax_plot.set_ylabel(y_label,fontsize=18)\n",
    "    mplt.rc('xtick', labelsize=15) \n",
    "    mplt.rc('ytick', labelsize=15)\n",
    "    #\n",
    "    median_labels = [str(s) for s in medians]\n",
    "    pos = range(num_models)\n",
    "    for tick,label in zip(pos,ax_plot.get_xticklabels()):\n",
    "        ax_plot.text(pos[tick], tope-var_tope, median_labels[tick], \n",
    "                horizontalalignment='center', color='black', fontsize=15) #, weight='semibold'\n",
    "    axes = plt.gca()\n",
    "    if column_plot == 'values':\n",
    "        axes.set_ylim([0.4, tope])\n",
    "    else:\n",
    "        axes.set_ylim([-0.1, tope])\n",
    "    plt.savefig(filename + \".eps\", bbox_inches='tight')    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Combinatorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling .... LogisticRegression\n",
      "Modeling .... LinearDiscriminantAnalysis\n",
      "Modeling .... GaussianNB\n",
      "Modeling .... MLPClassifier\n",
      "Modeling .... SVC\n"
     ]
    }
   ],
   "source": [
    "estimadores = set_models()\n",
    "configuraciones = set_config()\n",
    "score_cache = {}\n",
    "resultados = []\n",
    "lista_resultados = []\n",
    "for name,model in estimadores:\n",
    "    values = configuraciones[name]\n",
    "    getAccuracy(frecuencias, values, model, score_cache, resultados)\n",
    "    #lista_resultados = lista_resultados + list(a.resultados)\n",
    "    #lista_resultados.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelando ...  LinearDiscriminantAnalysis\n"
     ]
    }
   ],
   "source": [
    "topDf = pd.DataFrame(resultados)\n",
    "l_name = ['Modelo', 'Configuracion', 'Accuracy', 'stdAccuracy', 'errorMetrico']\n",
    "topDf[l_name].to_csv('resultadosTOP_top_gs.csv', sep=',', index=False)\n",
    "display(topDf[l_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_boxplot(topDf, 'values', 'acc_results_top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_boxplot(topDf, 'error', 'error_resultsTop_barplot', box_bool = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP EAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_config():\n",
    "    top_conf = {}\n",
    "    top_conf['LogisticRegression']\t=\tnp.array([4, 1, 2, 3, 1])-1    \n",
    "    top_conf['LinearDiscriminantAnalysis']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    top_conf['GaussianNB']\t=\tnp.array([4, 1, 2, 6, 1])-1\n",
    "    top_conf['MLPClassifier']\t=\tnp.array([4, 1, 4, 6, 1])-1\n",
    "    top_conf['SVC']\t=\tnp.array([6, 6, 3, 3, 5])-1 #3\n",
    "    top_conf['DecisionTreeClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1 #2\n",
    "    top_conf['KNeighborsClassifier']\t=\tnp.array([6, 1, 3, 6, 1])-1\n",
    "    top_conf['RandomForestClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    top_conf['ExtraTreesClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    top_conf['GradientBoostingClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    top_conf['AdaBoostClassifier']\t=\tnp.array([6, 1, 3, 3, 1])-1#6\n",
    "    top_conf['VotingClassifier']\t=\tnp.array([6, 1, 3, 3, 5])-1\n",
    "    return top_conf\n",
    "\n",
    "def set_models():\n",
    "    rs = 1\n",
    "    models = []\n",
    "    models.append(('LogisticRegression', LogisticRegression(n_jobs=-1, C=0.9, multi_class='multinomial', solver='newton-cg', warm_start=True)))\n",
    "    # LDA : Warning(Variables are collinear)\n",
    "    models.append(('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis(n_jobs=-1, solver='eigen')))    \n",
    "    models.append(('GaussianNB', GaussianNB(n_jobs=-1, priors=None)))\n",
    "    models.append(('MLPClassifier', MLPClassifier(n_jobs=-1, hidden_layer_sizes=100, alpha=1e-05, activation='tanh', solver='adam', learning_rate='invscaling')))\n",
    "    models.append(('SVC', SVC(n_jobs=-1, random_state=rs, C=1, kernel='rbf', decision_function_shape='ovo')))\n",
    "    models.append(('DecisionTreeClassifier', DecisionTreeClassifier(n_jobs=-1, random_state=rs, max_features=None, splitter='best', criterion='entropy', class_weight=None, max_depth=10)))\n",
    "    models.append(('KNeighborsClassifier', KNeighborsClassifier(n_jobs=-1, n_neighbors=5, weights='distance', algorithm='ball_tree')))\n",
    "    \n",
    "    models.append(('RandomForestClassifier', RandomForestClassifier(n_jobs=-1, random_state=rs, max_depth=20, class_weight=None, max_features='sqrt', criterion='gini', warm_start=True, n_estimators=30)))\n",
    "    # Bagging and Boosting\n",
    "    # models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=150)))\n",
    "    models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_jobs=-1, random_state=rs, max_depth=5, class_weight='balanced_subsample', min_samples_leaf=1, max_features='sqrt', max_leaf_nodes=9, criterion='gini', n_estimators=15)))\n",
    "    \n",
    "    # models.append(('AdaBoostClassifier', AdaBoostClassifier(DecisionTreeClassifier())))\n",
    "    \n",
    "    models.append(('GradientBoostingClassifier',GradientBoostingClassifier(n_jobs=-1, random_state=rs, max_features='sqrt', learning_rate=0.1, max_depth=3, n_estimators=100)))\n",
    "    models.append(('AdaBoostClassifier', AdaBoostClassifier(n_jobs=-1, DecisionTreeClassifier(random_state=rs, max_features=None, splitter='best', criterion='entropy', class_weight=None, max_depth=10), \n",
    "                                                            n_estimators=500, algorithm='SAMME.R', learning_rate=2.0, random_state=rs)))\n",
    "    # models.append(('GradientBoostingClassifier', GradientBoostingClassifier()))\n",
    "    # Voting\n",
    "    estimators = []\n",
    "    estimators.append((\"Voting_GradientBoostingClassifier\",GradientBoostingClassifier(random_state=rs, max_features='sqrt', learning_rate=0.1, max_depth=3, n_estimators=100)))\n",
    "    estimators.append((\"Voting_ExtraTreesClassifier\", ExtraTreesClassifier(random_state=rs, max_depth=5, class_weight='balanced_subsample', min_samples_leaf=1, max_features='sqrt', max_leaf_nodes=9, criterion='gini', n_estimators=15))) \n",
    "    voting = VotingClassifier(estimators, voting='soft', n_jobs=-1)\n",
    "    models.append(('VotingClassifier', voting))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimadores = set_models()\n",
    "configuraciones = set_config()\n",
    "score_cache = {}\n",
    "resultados2 = []\n",
    "lista_resultados2 = []\n",
    "for name,model in estimadores:\n",
    "    values = configuraciones[name]\n",
    "    getAccuracy(frecuencias, values, model, score_cache, resultados2)\n",
    "    #lista_resultados = lista_resultados + list(a.resultados)\n",
    "    #lista_resultados.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topDf = pd.DataFrame(resultados2)\n",
    "l_name = ['Modelo', 'Configuracion', 'Accuracy', 'stdAccuracy', 'errorMetrico']\n",
    "topDf[l_name].to_csv('resultadosEAS_top_gs.csv', sep=',', index=False)\n",
    "display(topDf[l_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_boxplot(topDf, 'values', 'acc_results_eas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_boxplot(topDf, 'error', 'error_resultsEAS_barplot', box_bool = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
