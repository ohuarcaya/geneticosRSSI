{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Librería Genética\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "# Subfunciones de estimadores\n",
    "from sklearn.base import clone\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py][30]\n",
    "from sklearn.base import is_classifier\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py][535]\n",
    "from sklearn.model_selection._validation import _fit_and_score\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_validation.py][346]\n",
    "from sklearn.model_selection._search import BaseSearchCV\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py][386]\n",
    "from sklearn.model_selection._search import check_cv\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_split.py][1866]\n",
    "from sklearn.model_selection._search import _check_param_grid\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py][343]\n",
    "from sklearn.metrics.scorer import check_scoring\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/scorer.py][250]\n",
    "from sklearn.utils.validation import _num_samples\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py][105]\n",
    "from sklearn.utils.validation import indexable\n",
    "# [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py][208]\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "# Selección para estimadores\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Metricas para estimadores\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Estimadores\n",
    "from sklearn.ensemble import ExtraTreesClassifier#\n",
    "from sklearn.ensemble import RandomForestClassifier#\n",
    "from sklearn.ensemble import AdaBoostClassifier#\n",
    "from sklearn.ensemble import GradientBoostingClassifier#\n",
    "from sklearn.tree import DecisionTreeClassifier#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\"\"\"\n",
    "PYMACH\n",
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Ensembles algorithms\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# find distance error al 0.2%\n",
    "def distance_error(estimator, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 7)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    # coord pred\n",
    "    x1 = np.int32((y_pred + 2) % 3)\n",
    "    y1 = np.int32((y_pred - 1) / 3)\n",
    "    # coord real\n",
    "    x2 = np.int32((y_test + 2) % 3)\n",
    "    y2 = np.int32((y_test - 1) / 3)\n",
    "    # pasar variacion a distancias metros\n",
    "    vx = np.abs(x1 - x2)\n",
    "    vy = np.abs(x1 - x2)\n",
    "    vx = vx*0.5 + (vx-1)*(vx>0)\n",
    "    vy = vy*0.5 + (vy-1)*(vy>0)\n",
    "    # pitagoras\n",
    "    err_distance = np.mean(np.sqrt(vx*vx + vy*vy))\n",
    "    return err_distance\n",
    "\n",
    "def _createDataset(frecuencias, values, seed = 7):\n",
    "    # crear dataset\n",
    "    names_ = frecuencias[0].columns.values\n",
    "    # reestructuracion\n",
    "    salida_final = pd.DataFrame(columns=names_)\n",
    "    for sec in range(1,16):\n",
    "        dataset = pd.DataFrame(columns=names_)\n",
    "        corte = min([frecuencias[i][frecuencias[i]['Sector']==sec].shape[0] for i in values])\n",
    "        tx = 0\n",
    "        dataset[names_[tx]] = dataset[names_[tx]].append(frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx]])\n",
    "        for tx in range(1,5):\n",
    "            dataset[names_[tx]] = frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx]]\n",
    "        dataset[names_[tx+1]] = frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx+1]]\n",
    "        # join parts\n",
    "        salida_final = salida_final.append(dataset)\n",
    "    # shuffle dataset\n",
    "    salida_final = shuffle(salida_final, random_state=seed).reset_index(drop=True)\n",
    "    salida_final = salida_final.apply(pd.to_numeric)\n",
    "    # dataframe to X,y \n",
    "    X = salida_final[names_[:-1]]\n",
    "    y = salida_final[names_[-1]]\n",
    "    return X,y\n",
    "\n",
    "def set_models():\n",
    "    rs = 1\n",
    "    models = []\n",
    "    # LDA : Warning(Variables are collinear)\n",
    "    models.append(('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis()))\n",
    "    models.append(('SVC', SVC(random_state=rs)))\n",
    "    models.append(('GaussianNB', GaussianNB()))\n",
    "    models.append(('MLPClassifier', MLPClassifier()))\n",
    "    models.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "    models.append(('DecisionTreeClassifier', DecisionTreeClassifier(random_state=rs)))\n",
    "    models.append(('LogisticRegression', LogisticRegression()))\n",
    "    # Bagging and Boosting\n",
    "    # models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=150)))\n",
    "    models.append(('ExtraTreesClassifier', ExtraTreesClassifier(random_state=rs)))\n",
    "    models.append(('AdaBoostClassifier', AdaBoostClassifier(DecisionTreeClassifier(random_state=rs),\n",
    "                                                            random_state=rs)))\n",
    "    # models.append(('AdaBoostClassifier', AdaBoostClassifier(DecisionTreeClassifier())))\n",
    "    models.append(('RandomForestClassifier', RandomForestClassifier(random_state=rs)))\n",
    "    models.append(('GradientBoostingClassifier',\n",
    "                   GradientBoostingClassifier(random_state=rs)))\n",
    "    # models.append(('GradientBoostingClassifier', GradientBoostingClassifier()))\n",
    "    # Voting\n",
    "    estimators = []\n",
    "    estimators.append((\"Voting_GradientBoostingClassifier\", GradientBoostingClassifier(random_state=rs)))\n",
    "    estimators.append((\"Voting_ExtraTreesClassifier\", ExtraTreesClassifier(random_state=rs)))\n",
    "    voting = VotingClassifier(estimators)\n",
    "    models.append(('VotingClassifier', voting))\n",
    "    return models\n",
    "\n",
    "\n",
    "# The problem to optimize\n",
    "def evaluate( frecuencias, individual, estimator, score_cache={}, error_cache={}, \n",
    "             n_splits = 10, shuffle = False, scorer = \"accuracy\"):\n",
    "    X, y = _createDataset(frecuencias, individual)\n",
    "    metric_err = distance_error(estimator, X, y)\n",
    "    score = 0\n",
    "    paramkey = str(np.int32(individual)+1)\n",
    "    if paramkey in score_cache:\n",
    "        score = score_cache[paramkey]\n",
    "        error = error_cache[paramkey]\n",
    "    else:\n",
    "        kfold = KFold(n_splits=n_splits, shuffle=shuffle)\n",
    "        cv_results = cross_val_score(estimator, X, y, cv=kfold, scoring=scorer)\n",
    "        score = cv_results.mean()\n",
    "        error = cv_results.std()\n",
    "        #score_cache[paramkey] = score\n",
    "        #error_cache[paramkey] = error\n",
    "    return score, error, metric_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _initIndividual(individuo, maxints):\n",
    "\t\"\"\"[Iniciar Individuo]\n",
    "\tArguments:\n",
    "\t\tpcls {[creator.Individual]} -- [Iniciar individuo con indices aleatorios]\n",
    "\t\tmaxints {[params_size]} -- [lista de máximos índices]\n",
    "\tReturns:\n",
    "\t\t[creator.Individual] -- [Creación de individuo]\n",
    "\t\"\"\"\n",
    "\treturn individuo(rnd.randint(0, maxint) for maxint in maxints)\n",
    "\n",
    "def _mutIndividual(individual, maxints, prob_mutacion):\n",
    "\t\"\"\"[Mutación Individuo]\n",
    "\tArguments:\n",
    "\t\tindividual {[creator.Individual]} -- [Individuo de población]\n",
    "\t\tmaxints {[lista]} -- [lista de máximos índices]\n",
    "\t\tprob_mutacion {[float]} -- [probabilidad de mutación del gen]\n",
    "\tReturns:\n",
    "\t\t[creator.Individual] -- [Individuo mutado]\n",
    "\t\"\"\"\n",
    "\tfor i in range(len(maxints)):\n",
    "\t\tif rnd.random() < prob_mutacion:\n",
    "\t\t\tindividual[i] = rnd.randint(0, maxints[i])\n",
    "\treturn individual,\n",
    "\n",
    "def _cxIndividual(ind1, ind2, prob_cruce):\n",
    "\t\"\"\"[Cruce de Individuos]\n",
    "\tArguments:\n",
    "\t\tind1 {[creator.Individual]} -- [Individuo 1]\n",
    "\t\tind2 {[creator.Individual]} -- [Individuo 2]\n",
    "\t\tindpb {[float]} -- [probabilidad de emparejar]\n",
    "\t\tgene_type {[list]} -- [tipos de dato de los parámetros, CATEGORICO o NUMERICO]\n",
    "\tReturns:\n",
    "\t\t[creator.Individual,creator.Individual] -- [nuevos Individuos]\n",
    "\t\"\"\"\n",
    "\tCATEGORICO = 1  # int o str\n",
    "\tNUMERICO = 2  # float\n",
    "\tfor i in range(len(ind1)):\n",
    "\t\tif rnd.random() < prob_cruce:\n",
    "\t\t\tsorted_ind = sorted([ind1[i], ind2[i]])\n",
    "\t\t\tind1[i] = rnd.randint(sorted_ind[0], sorted_ind[1])\n",
    "\t\t\tind2[i] = rnd.randint(sorted_ind[0], sorted_ind[1])\n",
    "\treturn ind1, ind2\n",
    "\n",
    "\"\"\"\n",
    "def _individual_to_params(individual, frecuencias):\n",
    "\tnames_ = frecuencias[0].columns.values\n",
    "\tdataset = pd.DataFrame()\n",
    "\tdataset[names_[0]] = frecuencias[individual[0]][names_[0]]\n",
    "\tdataset[names_[1]] = frecuencias[individual[1]][names_[1]]\n",
    "\tdataset[names_[2]] = frecuencias[individual[2]][names_[2]]\n",
    "\tdataset[names_[3]] = frecuencias[individual[3]][names_[3]]\n",
    "\tdataset[names_[4]] = frecuencias[individual[4]][names_[4]]\n",
    "\tdataset[names_[5]] = frecuencias[0][names_[5]]\n",
    "\t# separación de data en X,y \n",
    "\ty = dataset[names_[5]]\n",
    "\tdel dataset[names_[5]]\n",
    "\tX = dataset\n",
    "\treturn X,y\n",
    "\"\"\"\n",
    "def _individual_to_params(frecuencias, values, seed = 7):\n",
    "    # crear dataset\n",
    "    names_ = ['Be01', 'Be02', 'Be03', 'Be04', 'Be05', 'Sector']#frecuencias[0].columns.values\n",
    "    # reestructuracion\n",
    "    salida_final = pd.DataFrame(columns=names_)\n",
    "    for sec in range(1,16):\n",
    "        dataset = pd.DataFrame(columns=names_)\n",
    "        corte = min([frecuencias[i][frecuencias[i]['Sector']==sec].shape[0] for i in values])\n",
    "        tx = 0\n",
    "        dataset[names_[tx]] = dataset[names_[tx]].append(frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx]])\n",
    "        for tx in range(1,5):\n",
    "            dataset[names_[tx]] = frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx]]\n",
    "        dataset[names_[tx+1]] = frecuencias[int(values[tx])][frecuencias[int(values[tx])]['Sector']==sec][:corte][names_[tx+1]]\n",
    "        # join parts\n",
    "        salida_final = salida_final.append(dataset)\n",
    "    # shuffle dataset\n",
    "    salida_final = shuffle(salida_final, random_state=seed).reset_index(drop=True)\n",
    "    salida_final = salida_final.apply(pd.to_numeric)\n",
    "    # dataframe to X,y \n",
    "    X = salida_final[names_[:-1]]\n",
    "    y = salida_final[names_[-1]]\n",
    "    return X,y\n",
    "\n",
    "def _evalFunction(individual, frecuencias, scorer, num_folds, score_cache, desv_cache, error_cache):\n",
    "\t\"\"\"[Evaluación del modelo]\n",
    "\tArguments:\n",
    "\t\tindividual {[creator.Individual]} -- [Individuo]\n",
    "\t\tfrecuencias {[list]} -- [lista de dataframes]\n",
    "\t\tX {[array]} -- [Input]\n",
    "\t\ty {[array]} -- [Output]\n",
    "\t\tscorer {[string]} -- [Parámetro de evaluación, precisión]\n",
    "\t\tcv {[int | cross-validation]} -- [Especificación de los folds]\n",
    "\t\tuniform {[boolean]} -- [True hace que la data se distribuya uniformemente en los folds]\n",
    "\t\tfit_params {[dict | None]} -- [parámetros para estimator.fit]\n",
    "\tKeyword Arguments:\n",
    "\t\tverbose {integer} -- [Mensajes de descripción] (default: {0})\n",
    "\t\terror_score {numerico} -- [valor asignado si ocurre un error en fitting] (default: {'raise'})\n",
    "\t\tscore_cache {dict} -- [description] (default: {{}})\n",
    "\t\"\"\"\n",
    "\tX, y = _individual_to_params(individual, frecuencias)\n",
    "\tscore = 0\n",
    "\tn_test = 0\n",
    "\tparamkey = str(individual.est).split('(')[0] + \"-\" + str(np.array(individual)+1)\n",
    "\tif paramkey in score_cache:\n",
    "\t\tscore = score_cache[paramkey]\n",
    "\telse:\n",
    "\t\tkfold = KFold(n_splits=10, shuffle=False)\n",
    "\t\t#cv_results = cross_val_score(estimator, X, y, cv=kfold, scoring=scoring)\n",
    "\t\tcv_results = cross_val_score(individual.est, X, y, cv=kfold, scoring=scorer)\n",
    "\t\tscore = cv_results.mean()\n",
    "\t\tdesv = cv_results.std()\n",
    "\t\terror = distance_error(individual.est, X, y)\n",
    "\t\tscore_cache[paramkey] = score\n",
    "\t\tdesv_cache[paramkey] = desv\n",
    "\t\terror_cache[paramkey] = error\n",
    "\treturn (score,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-13-c7cade60b7b1>, line 110)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-c7cade60b7b1>\"\u001b[0;36m, line \u001b[0;32m110\u001b[0m\n\u001b[0;31m    (individual, frecuencias, scorer, num_folds, score_cache, desv_cache, error_cache):\u001b[0m\n\u001b[0m                                                                                       ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "class EvolutiveSearchCV:\n",
    "\tdef __init__(self, estimator, scoring=None, num_folds=4,\n",
    "\t\t\t\trefit=True, verbose=False, population_size=50,\n",
    "\t\t\t\tgene_mutation_prob=0.2, gene_crossover_prob=0.5,\n",
    "\t\t\t\ttournament_size=3, generations_number=10, gene_type=None,\n",
    "\t\t\t\tn_jobs=1, uniform=True, error_score='raise',\n",
    "\t\t\t\tfit_params={}):\n",
    "\t\t# Parámetros iniciales\n",
    "\t\tself.estimator = estimator\n",
    "\t\t#self.params = params\n",
    "\t\tself.scoring = scoring\n",
    "\t\tself.num_folds = num_folds\n",
    "\t\tself.refit = refit\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.population_size = population_size\n",
    "\t\tself.gene_mutation_prob = gene_mutation_prob\n",
    "\t\tself.gene_crossover_prob = gene_crossover_prob\n",
    "\t\tself.tournament_size = tournament_size\n",
    "\t\tself.generations_number = generations_number\n",
    "\t\tself.gene_type = gene_type\n",
    "\t\tself.n_jobs = n_jobs\n",
    "\t\tself.uniform = uniform\n",
    "\t\tself.error_score = error_score\n",
    "\t\tself.fit_params = fit_params\n",
    "\t\t# Parámetros adicionales\n",
    "\t\tself._individual_evals = {}\n",
    "\t\tself.all_history_ = None\n",
    "\t\tself.all_logbooks_ = None\n",
    "\t\tself._cv_results = None\n",
    "\t\tself.best_score_ = None\n",
    "\t\tself.best_params_ = None\n",
    "\t\tself.scorer_ = None\n",
    "\t\t#self.score_cache = {}\n",
    "\t\tself.__manager = Manager()\n",
    "\t\tself.score_cache = self.__manager.dict()\n",
    "\t\tself.desv_cache = self.__manager.dict()\n",
    "\t\tself.error_cache = self.__manager.dict()\n",
    "\t\t# Fitness [base.Fitness], objetivo 1\n",
    "\t\tcreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "\t\t# Individuo [list], parámetros:est, FinessMax\n",
    "\t\tcreator.create(\"Individual\", list, est=clone(self.estimator), fitness=creator.FitnessMax)\n",
    "\t#@property\n",
    "\tdef cv_results_(self):\n",
    "\t\tif self._cv_results is None:\n",
    "\t\t\tout = defaultdict(list)\n",
    "\t\t\tgen = self.all_history_\n",
    "\t\t\t# Get individuals and indexes, their list of scores,\n",
    "\t\t\t# and additionally the name_values for this set of parameters\n",
    "\t\t\tidxs, individuals, each_scores = zip(*[(idx, indiv, np.mean(indiv.fitness.values))\n",
    "\t\t\t\t\t\t\t\t\t\t\tfor idx, indiv in list(gen.genealogy_history.items())\n",
    "\t\t\t\t\t\t\t\t\t\t\tif indiv.fitness.valid and not np.all(np.isnan(indiv.fitness.values))])\n",
    "\t\t\t#name_values, _, _ = _get_param_types_maxint(self.params)\n",
    "\t\t\t# Add to output\n",
    "\t\t\t#out['param_index'] += [p] * len(idxs)\n",
    "\t\t\tout['index'] += idxs\n",
    "\t\t\t#out['params'] += [_individual_to_params(indiv, name_values) for indiv in individuals]\n",
    "\t\t\tout['params'] += [str(np.add(indiv,1)) for indiv in individuals]\n",
    "\t\t\tout['mean_test_score'] += [np.nanmean(scores) for scores in each_scores]\n",
    "\t\t\tout['std_test_score'] += [np.nanstd(scores) for scores in each_scores]\n",
    "\t\t\tout['min_test_score'] += [np.nanmin(scores) for scores in each_scores]\n",
    "\t\t\tout['max_test_score'] += [np.nanmax(scores) for scores in each_scores]\n",
    "\t\t\tout['nan_test_score?'] += [np.any(np.isnan(scores)) for scores in each_scores]\n",
    "\t\t\tself._cv_results = out\n",
    "\t\treturn self._cv_results\n",
    "\t@property\n",
    "\tdef best_index_(self):\n",
    "\t\treturn np.argmax(self.cv_results_['max_test_score'])\n",
    "\t# fit y refit general\n",
    "\tdef fit(self, frecuencias):\n",
    "\t\tself.best_estimator_ = None\n",
    "\t\tself.best_mem_score_ = float(\"-inf\")\n",
    "\t\tself.best_mem_params_ = None\n",
    "\t\t#_check_param_grid(self.params)\n",
    "\t\tself._fit(frecuencias)\n",
    "\t\t#if self.refit:\n",
    "\t\t#\tself.best_estimator_ = clone(self.estimator)\n",
    "\t\t#\t#self.best_estimator_.set_params(**self.best_mem_params_)\n",
    "\t\t#\tself.best_estimator_.fit(frecuencias)\n",
    "\t# fit individual\n",
    "\tdef _fit(self, frecuencias):\n",
    "\t\tself._cv_results = None  # Indicador de necesidad de actualización\n",
    "\t\tself.scorer_ = check_scoring(self.estimator, scoring=self.scoring)\n",
    "\t\t#n_samples = _num_samples(X)\n",
    "\t\t# verificar longitudes x,y \n",
    "\t\t#if _num_samples(y) != n_samples:\n",
    "\t\t#\traise ValueError('Target [y], data [X] dont agree')\n",
    "\t\t#cv = check_cv(self.cv, y=y, classifier=is_classifier(self.estimator))\n",
    "\t\ttoolbox = base.Toolbox()\n",
    "\t\t# name_values = lista de parametros, gene_type = [1:categorico; 2:numérico], maxints = size(parametros)\n",
    "\t\t#name_values, self.gene_type, maxints = _get_param_types_maxint(parameter_dict)\n",
    "\t\tmaxints = [5]*5\n",
    "\t\t#if self.verbose:\n",
    "\t\t#\tprint(\"Tipos: %s, rangos: %s\" % (self.gene_type, maxints))\n",
    "\t\t# registro de función Individuo\n",
    "\t\ttoolbox.register(\"individual\", _initIndividual, creator.Individual, maxints=maxints)\n",
    "\t\t# registro de función Población\n",
    "\t\ttoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\t\t# Paralelísmo, create pool\n",
    "\t\tif not isinstance(self.n_jobs, int):\n",
    "\t\t\tself.n_jobs=1\n",
    "\t\tpool = Pool(self.n_jobs)\n",
    "\t\ttoolbox.register(\"map\", pool.map)\n",
    "\t\t# registro de función Evaluación\n",
    "\t\ttoolbox.register(\"evaluate\", _evalFunction,\n",
    "\t\t\t\t\t\tfrecuencias=frecuencias,\n",
    "\t\t\t\t\t\tscorer=self.scorer_, num_folds=10, \n",
    "\t\t\t\t\t\tscore_cache=self.score_cache,\n",
    "\t\t\t\t\t\tdesv_cache=self.desv_cache,\n",
    "\t\t\t\t\t\terror_cache=self.error_cache)\n",
    "        (individual, frecuencias, scorer, num_folds, score_cache, desv_cache, error_cache):\n",
    "\t\t# registro de función Cruce\n",
    "\t\ttoolbox.register(\"mate\", _cxIndividual, prob_cruce=self.gene_crossover_prob)\n",
    "\t\t# registro de función Mutación\n",
    "\t\ttoolbox.register(\"mutate\", _mutIndividual, prob_mutacion=self.gene_mutation_prob, maxints=maxints)\n",
    "\t\t# registro de función Selección\n",
    "\t\ttoolbox.register(\"select\", tools.selTournament, tournsize=self.tournament_size)\n",
    "\t\t# Creación de Población\n",
    "\t\tpop = toolbox.population(n=self.population_size)\n",
    "\t\t# Mejor Individuo que ha existido\n",
    "\t\thof = tools.HallOfFame(1)\n",
    "\t\t# Stats\n",
    "\t\tstats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "\t\tstats.register(\"avg\", np.nanmean)\n",
    "\t\tstats.register(\"min\", np.nanmin)\n",
    "\t\tstats.register(\"max\", np.nanmax)\n",
    "\t\tstats.register(\"std\", np.nanstd)\n",
    "\t\t# Genealogía\n",
    "\t\thist = tools.History()\n",
    "\t\t# Decoración de operadores de variaznza\n",
    "\t\ttoolbox.decorate(\"mate\", hist.decorator)\n",
    "\t\ttoolbox.decorate(\"mutate\", hist.decorator)\n",
    "\t\thist.update(pop)\n",
    "\t\t# Posibles combinaciones\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint('--- Evolve in {0} possible combinations ---'.format(np.prod(np.array(maxints) + 1)))\n",
    "\t\tpop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=self.gene_crossover_prob, \n",
    "\t\t\t\t\t\t\t\t\t\tmutpb=self.gene_mutation_prob,\n",
    "\t\t\t\t\t\t\t\t\t\tngen=self.generations_number, \n",
    "\t\t\t\t\t\t\t\t\t\tstats=stats,\n",
    "\t\t\t\t\t\t\t\t\t\thalloffame=hof, \n",
    "\t\t\t\t\t\t\t\t\t\tverbose=self.verbose)\n",
    "\t\t#pop, logbook = algorithms.eaGenerateUpdate(toolbox,\n",
    "\t\t#\t\t\t\t\t\t\t\tngen=self.generations_number, stats=stats,\n",
    "\t\t#\t\t\t\t\t\t\t\thalloffame=hof, verbose=self.verbose)\n",
    "\t\t# Save History\n",
    "\t\tself.all_history_ = hist\n",
    "\t\tself.all_logbooks_ = logbook\n",
    "\t\t# Mejor score y parametros\n",
    "\t\tcurrent_best_score_ = hof[0].fitness.values[0]\n",
    "\t\tcurrent_best_params_ = str(hof[0]) #_individual_to_params(hof[0], name_values)\n",
    "\t\t#if self.verbose:\n",
    "\t\t#\tprint(\"Best individual is: %s\\nwith fitness: %s\" % (\n",
    "\t\t#\t\tcurrent_best_params_, current_best_score_))\n",
    "\t\tif current_best_score_ > self.best_mem_score_:\n",
    "\t\t\tself.best_mem_score_ = current_best_score_\n",
    "\t\t\tself.best_mem_params_ = current_best_params_\n",
    "\t\t# fin paralelización, close pool\n",
    "\t\tpool.close()\n",
    "\t\tpool.join()\n",
    "\t\tself.best_score_ = current_best_score_\n",
    "\t\tself.best_params_ = current_best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "frecuencias = []\n",
    "names_ = ['Be01', 'Be02', 'Be03', 'Be04', 'Be05', 'Sector']\n",
    "\"\"\"\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x01'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x02'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x03'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x04'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x05'))#, names=names_))\n",
    "frecuencias.append(pd.read_csv('sinFiltro/Tx_0x06'))#, names=names_))\n",
    "\"\"\"\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx1.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx2.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx3.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx4.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx5.csv', names=names_))\n",
    "frecuencias.append(pd.read_csv('Filtrado/LocalizationNew_Tx6.csv', names=names_))\n",
    "\n",
    "num_jobs=4\n",
    "estimadores = set_models()\n",
    "salida = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modeling... LinearDiscriminantAnalysis\n",
      "--- Evolve in 7776 possible combinations ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ohuarcaya/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/ohuarcaya/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-12-4fae7fdedd7b>\", line 99, in _evalFunction\n    X, y = _individual_to_params(individual, frecuencias)\n  File \"<ipython-input-12-4fae7fdedd7b>\", line 67, in _individual_to_params\n    corte = min([frecuencias[i][frecuencias[i]['Sector']==sec].shape[0] for i in values])\n  File \"<ipython-input-12-4fae7fdedd7b>\", line 67, in <listcomp>\n    corte = min([frecuencias[i][frecuencias[i]['Sector']==sec].shape[0] for i in values])\nTypeError: list indices must be integers or slices, not DataFrame\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-75ffe5e3ba94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \u001b[0mtournament_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                         generations_number=12)\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrecuencias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mreserva\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesv_cache\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-21434fa9195d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, frecuencias)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_mem_params_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m#_check_param_grid(self.params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrecuencias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;31m#if self.refit:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m#       self.best_estimator_ = clone(self.estimator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-21434fa9195d>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, frecuencias)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                                                                 \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                                                                 \u001b[0mhalloffame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhof\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m \t\t\t\t\t\t\t\t\t\tverbose=self.verbose)\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0;31m#pop, logbook = algorithms.eaGenerateUpdate(toolbox,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;31m#                                                               ngen=self.generations_number, stats=stats,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/deap/algorithms.py\u001b[0m in \u001b[0;36meaSimple\u001b[0;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Evaluate the individuals with an invalid fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not DataFrame"
     ]
    }
   ],
   "source": [
    "def set_models():\n",
    "    rs = 1\n",
    "    models = []\n",
    "    models.append(('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "    return models\n",
    "estimadores = set_models()\n",
    "reserva = {}\n",
    "for name, model in estimadores:\n",
    "    print(\"\\nModeling...\", name)\n",
    "    splits = 10\n",
    "    simetricas = [[i]*5 for i in range(6)]\n",
    "    #for individual in simetricas:\n",
    "    #acc, desv, err = evaluate(frecuencias, individual, model)\n",
    "    #salida[str(name)+\"-\"+str(individual)] = str(acc) + \"-\"+ str(desv) + \"-\" + str(err)\n",
    "    #print(name,\" \", individual, \"\\t\", acc, \"\\t\", desv, \"\\t\", err)\n",
    "    gs = EvolutiveSearchCV(estimator=model, scoring=\"accuracy\", num_folds=10, n_jobs=num_jobs,\n",
    "                        verbose=True, refit=True, \n",
    "                        population_size=100, \n",
    "                        gene_mutation_prob=0.3, \n",
    "                        gene_crossover_prob=0.5,\n",
    "                        tournament_size=4,\n",
    "                        generations_number=12)\n",
    "    gs.fit(frecuencias)\n",
    "    reserva[name]=(gs.score_cache, gs.desv_cache , gs.error_cache)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Be01', 'Be02', 'Be03', 'Be04', 'Be05', 'Sector']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frecuencias[1].columns.values\n",
    "names_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
